{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMGO5M6uGfmj7OBBnwjsRQN",
      "include_colab_link": true,
      "name": "Med_LLM.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": { "state": {} }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": { "id": "open-in-colab" },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sama-Borhani/medical-llm-chatbot-lora-faiss-gradio/blob/main/notebooks/Med_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "title" },
      "source": [
        "# 🩺 Medical LLM Chatbot — RAG + Gradio (Demo)\n",
        "\n",
        "> **Educational use only — not medical advice.**\n",
        "\n",
        "This Colab notebook sets up dependencies, builds a tiny FAISS index from sample text (or your own files), loads `Writer/camel-5b-hf` in 4-bit/8-bit, and launches a Gradio demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "install" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install required packages (quiet)\n",
        "!pip -q install transformers accelerate bitsandbytes torch\n",
        "!pip -q install sentence-transformers faiss-cpu\n",
        "!pip -q install gradio PyPDF2 requests peft jinja2 python-dotenv\n",
        "import torch, os, sys, json, time, re, requests, faiss, numpy as np\n",
        "from pathlib import Path\n",
        "print('GPU available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "env" },
      "source": [
        "## 🔑 Environment\n",
        "Set a Hugging Face token **only if** your model needs it. (Camel-5B is public; you can leave blank.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "env-code" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "HF_TOKEN = ''  # put your token if required\n",
        "MODEL_ID = 'Writer/camel-5b-hf'\n",
        "EMBED_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "TOP_K = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "model" },
      "source": [
        "## 🧠 Load base model (4-bit → fallback 8-bit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "model-code" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_auth_token=HF_TOKEN or None)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "                         bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16)\n",
        "try:\n",
        "    print('Loading 4-bit...')\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID, device_map='auto', quantization_config=bnb,\n",
        "        use_auth_token=HF_TOKEN or None, torch_dtype=torch.float16\n",
        "    )\n",
        "except Exception as e:\n",
        "    print('4-bit failed:', e, '\\nRetrying 8-bit...')\n",
        "    bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID, device_map='auto', quantization_config=bnb,\n",
        "        use_auth_token=HF_TOKEN or None, torch_dtype=torch.float16\n",
        "    )\n",
        "device = mdl.device\n",
        "print('Model loaded on', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "corpus" },
      "source": [
        "## 📚 Corpus (add your own or use samples)\n",
        "We’ll create two tiny sample files if `corpus/` is empty. You can also upload your own `.txt` files in the sidebar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "corpus-code" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "corpus = Path('corpus'); corpus.mkdir(exist_ok=True)\n",
        "if not any(corpus.glob('*.txt')):\n",
        "    (corpus/'hypertension_basics.txt').write_text(\n",
        "        'Hypertension is persistently elevated blood pressure. Lifestyle changes and, when needed, ACEi/ARB, thiazides, or CCBs are common first-line choices.'\n",
        "    )\n",
        "    (corpus/'type2_diabetes_summary.txt').write_text(\n",
        "        'Type 2 diabetes is characterized by insulin resistance. Lifestyle changes and metformin are first-line; other agents depend on comorbidities.'\n",
        "    )\n",
        "    print('Added two sample docs to corpus/.')\n",
        "print('Corpus files:', [p.name for p in corpus.glob('*.txt')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "faiss" },
      "source": [
        "## 🔎 Build in-memory FAISS index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "faiss-code" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "texts, meta = [], []\n",
        "for p in sorted(corpus.glob('*.txt')):\n",
        "    t = p.read_text(encoding='utf-8', errors='ignore').strip()\n",
        "    if t:\n",
        "        texts.append(t)\n",
        "        meta.append({'path': str(p), 'title': p.stem})\n",
        "assert texts, 'No text found in corpus/. Add .txt files.'\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "emb = embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "index = faiss.IndexFlatIP(emb.shape[1]); index.add(emb)\n",
        "print('Index size:', index.ntotal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "retrieval" },
      "source": [
        "## 🧩 Retrieval + Prompting helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "helpers-code" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from jinja2 import Template\n",
        "SYSTEM = (\n",
        "    'You are a careful medical assistant for educational purposes only. '\n",
        "    'Answer briefly and cite snippet numbers like [#]. If unsure, say so.'\n",
        ")\n",
        "TEMPLATE = Template(\n",
        "    \"\"\"\n",
        "{{system}}\n",
        "\n",
        "Context:\n",
        "{% for i,s in sources %}[{{i+1}}] {{s[:220]}}...\\n{% endfor %}\n",
        "\n",
        "Question: {{q}}\n",
        "Answer with: 1) Summary, 2) Bullet points, 3) References by [#].\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "def retrieve(query, k=TOP_K):\n",
        "    qv = embedder.encode([query], normalize_embeddings=True).astype('float32')\n",
        "    sc, ix = index.search(qv, k)\n",
        "    items = []\n",
        "    for j in ix[0].tolist():\n",
        "        if 0 <= j < len(texts):\n", 
        "            items.append((texts[j], meta[j]))\n",
        "    return items\n",
        "\n",
        "def generate_answer(question):\n",
        "    srcs = retrieve(question)\n",
        "    prompt = TEMPLATE.render(system=SYSTEM, q=question, sources=list(enumerate([s for s,_ in srcs])))\n",
        "    toks = tok(prompt, return_tensors='pt').to(device)\n",
        "    out = mdl.generate(**toks, max_new_tokens=300, temperature=0.2, do_sample=False)\n",
        "    text = tok.decode(out[0], skip_special_tokens=True)\n",
        "    # Append simple refs\n",
        "    refs = '\\n\\nReferences: ' + ', '.join(f\"[{i+1}] {m['title']}\" for i,(_,m) in enumerate(srcs))\n",
        "    return text + refs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "gradio" },
      "source": [
        "## 🚀 Launch Gradio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "gradio-code" },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "def chat_fn(msg, history):\n",
        "    try:\n",
        "        ans = generate_answer(msg)\n",
        "    except Exception as e:\n",
        "        ans = f\"Error: {e}\\nTry a simpler question.\"\n",
        "    history = (history or []) + [[msg, ans + '\\n\\n⚠️ Not medical advice.']]\n",
        "    return history, ''\n",
        "\n",
        "with gr.Blocks(title='Medical LLM Chatbot (Demo)') as demo:\n",
        "    gr.Markdown('# Medical LLM Chatbot — Demo\\n*Educational use only*')\n",
        "    chat = gr.Chatbot(height=400)\n",
        "    msg = gr.Textbox(label='Ask a medical question', lines=2)\n",
        "    send = gr.Button('Send')\n",
        "    clear = gr.Button('Clear')\n",
        "    send.click(chat_fn, [msg, chat], [chat, msg])\n",
        "    msg.submit(chat_fn, [msg, chat], [chat, msg])\n",
        "    clear.click(lambda: [], None, chat)\n",
        "demo.launch(share=True)"
      ]
    }
  ]
}
